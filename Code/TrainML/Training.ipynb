{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNZIAb3l7iJv"
   },
   "source": [
    "# **PREREQUISITES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrycYzZ1xSPl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "import joblib\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from sklearn import preprocessing\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkConf, SparkContext, sql\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "from pyspark.sql.functions import stddev, stddev_pop, mean\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from math import sqrt, ceil\n",
    "from os import environ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the exact version of elasticsearch-spark-20_2.11-7.15.0.jar and place it inside the Apache Spark folder ##\n",
    "## The version should match the Elasticsearch version deployed in your system\n",
    "## https://www.elastic.co/downloads/hadoop\n",
    "## Modify the code below to point to the right path in your system\n",
    "## This jar is used for the hadoop implementation of elasticsearch index query and insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ['PYSPARK_SUBMIT_ARGS'] = '--jars \"/home/orchestrator/spark-2.3.0-bin-hadoop2.7/elasticsearch-spark-20_2.11-7.15.0.jar\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRXDfeZS8vje"
   },
   "outputs": [],
   "source": [
    "# Initialize global variables.\n",
    "kmeansModel = None\n",
    "knnModel = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH7CqJPe0VXV"
   },
   "outputs": [],
   "source": [
    "#setup configuration property \n",
    "#set the master URL \n",
    "#set an application name \n",
    "conf = SparkConf().setMaster(\"local[10]\").setAppName(\"sparkproject\")\n",
    "#start spark cluster \n",
    "#if already started then get it else start it \n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "#initialize SQLContext from spark cluster \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE_SMo41MdLN"
   },
   "source": [
    "# **DATA PROCESSING AND STANDARIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the elasticsearch query for obtaining the training DATA ##\n",
    "## Be sure to point to the correct address and port of the elasticsearch deployment ##\n",
    "## Change index name if needed ##\n",
    "## User name and password if using securyty enable cluster -- Change or erase if needed ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =\"\"\"{\n",
    "  \"query\": {\n",
    "    \"match_all\": {}\n",
    "  }  \n",
    "}\"\"\"\n",
    "\n",
    "es_read_conf = {\n",
    "\t\t\"es.nodes\" : 'localhost',\n",
    "\t\t# specify the port in case it is not the default port\n",
    "\t\t\"es.port\" : '9200',\n",
    "\t\t# specify a resource in the form 'index/doc-type'\n",
    "\t\t\"es.resource\" : 'sflowtest',\n",
    "        #Query\n",
    "        \"es.query\" : q,\n",
    "\t\t# is the input JSON?\n",
    "\t\t\"es.input.json\" : \"yes\",\n",
    "\t\t# is there a field in the mapping that should be used to specify the ES document ID\n",
    "\t\t#\"es.mapping.id\": round(time.time() * 1000),\n",
    "\t\t'es.net.http.auth.user' : 'elastic',\n",
    "\t\t'es.net.http.auth.pass' : 'elastic'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query for elasticsearch data\n",
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =sqlContext.createDataFrame(es_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the Dataframe into proper Feature columns\n",
    "df=df.rdd.map(lambda x: \\\n",
    "    (x._2[\"sumOfBytes\"],x._2[\"sumOfFlows\"], x._2[\"sumOfPackets\"],x._2[\"uniqDstIPs\"], x._2[\"uniqDstPorts\"] )) \\\n",
    "    .toDF(['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01cT_4om2Jra"
   },
   "outputs": [],
   "source": [
    "FEATURE_COLS = ['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xExNUJGR11wm",
    "outputId": "0c6d80b0-7fa9-4cec-b887-6ea432fd382d"
   },
   "outputs": [],
   "source": [
    "\t# Add unique numeric ID, and place in first column.\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = df.select(\"id\", FEATURE_COLS[0], FEATURE_COLS[1], FEATURE_COLS[2],FEATURE_COLS[3],FEATURE_COLS[4])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8tlzUGc2y1o",
    "outputId": "7acbdb6a-ce84-4984-fc9e-99076d505d83"
   },
   "outputs": [],
   "source": [
    "\t# Convert all data columns to float.\n",
    "df = df.select('id', *[col(c).cast('float').alias(c) for c in FEATURE_COLS])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Below code is used if you want to save this data as CSV **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the dataset as csv\n",
    "df.repartition(1).write.csv(\"cc_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark ML works with vectorized data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ES3TfYg2pUo",
    "outputId": "ef09a98b-829b-4f12-dd75-5c8f66886a5b"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=FEATURE_COLS, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')  # Drop other columns.\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr2q8vu6vndo"
   },
   "source": [
    "**Scale the data by using StandardScaler on a Vector of Features. WE NEED TO SAVE THIS TRAINED MODEL FOR FUTURE TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiJONKY43A9U",
    "outputId": "3641a73f-2ce8-4a0e-eb06-9f45d860ebbd"
   },
   "outputs": [],
   "source": [
    "# Scale the data.\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_kmeans) #Remeber this model and use it for new data\n",
    "df_scaled = scaler_model.transform(df_kmeans)\n",
    "df_scaled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The standardscaler model needs to be saved for later use in the spark enviroment - REMEBER THE PATH ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the Scaler model to file\n",
    "scaler_model.write().overwrite().save('scaler.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xAbqPOq-HTh"
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "\t# From https://stackoverflow.com/questions/38384347/how-to-split-vector-into-columns-using-pyspark\n",
    "\treturn (row[\"id\"], row[\"prediction\"], row[\"scaledFeatures\"]) + tuple(row['scaledFeatures'].toArray().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItZc-mkZMpyu"
   },
   "source": [
    "# **K-MEANS CLUSTERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgMbUP5Lv1vo"
   },
   "source": [
    "**Find the optimal Value of K by running a silhouette score evaluator for up to 20 clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixbK4_ma3RvR",
    "outputId": "6f3492fb-a5cf-4df4-8007-51b8b9990b08"
   },
   "outputs": [],
   "source": [
    "\t\t# # # # Find optimal choice for k.\n",
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "  kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"scaledFeatures\")\n",
    "  model = kmeans.fit(df_scaled)\n",
    "  predictions = model.transform(df_scaled).select('id', 'scaledFeatures', 'prediction')\n",
    "  predictions.show()\n",
    "  # Extract scaledFeatures column back to FEATURE_COLS\n",
    "  predictions = predictions.rdd.map(extract).toDF([\"id\", \"prediction\", \"scaledFeatures\", 'sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts'])\n",
    "  predictions.show()\n",
    "\t# Rename scaledFeatures to features.\n",
    "  predictions = predictions.withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "  predictions.show()\n",
    "  print(\"Prediction counts for each cluster:\")\n",
    "  predictions.groupBy('prediction').count().show()\n",
    "  evaluator = ClusteringEvaluator()\n",
    "  silhouette = evaluator.evaluate(predictions)\n",
    "  cost[k] = silhouette\n",
    "print(\" Cost =\")\n",
    "for k in range(2, 20):\n",
    "  print(\"{0}: {1}\".format(k, cost[k]))\n",
    "  print(\"Silhouette with squared euclidean distance = \" + str(cost[k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5SjK4MAwAkW"
   },
   "source": [
    "**Plot the optimal K value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "pO0lMuh1Dour",
    "outputId": "e6ad9a8c-20cd-4551-cf3f-b713a3d1fcf2"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20),cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVyKwL0XwDtI"
   },
   "source": [
    "**Train K-means using the optimal K**\n",
    "** Change the value of K to the best score value obtained from previous code block **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdF9HFS0-3I8",
    "outputId": "37edce12-8da6-4fa4-c3c7-4c7cf8d93775"
   },
   "outputs": [],
   "source": [
    "k = 14  \n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"scaledFeatures\")\n",
    "model = kmeans.fit(df_scaled)\n",
    "\t\t\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "  print(center)\n",
    "\t\t\n",
    "# Assign events to clusters. Testing the model\n",
    "predictions = model.transform(df_scaled).select('id', 'scaledFeatures', 'prediction')\n",
    "\t\t\n",
    "predictions.show()\n",
    "\t\t\n",
    "# Extract scaledFeatures column back to FEATURE_COLS\n",
    "predictions = predictions.rdd.map(extract).toDF([\"id\", \"prediction\", \"scaledFeatures\", 'sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts'])\n",
    "\t\t\n",
    "# Rename scaledFeatures to features.\n",
    "predictions = predictions.withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "df_pred = predictions\n",
    "df_pred.show()\n",
    "df_pred_plot = df_pred.drop('features')\n",
    "df_pred_plot.show()\n",
    "print(\"Prediction counts for each cluster:\")\n",
    "predictions.groupBy('prediction').count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "D0TtD-aXF4ON",
    "outputId": "6762cf60-1350-4c20-bc2c-0397d0d9571d"
   },
   "outputs": [],
   "source": [
    "###convert dataframe to pandas for visualization\n",
    "pddf_pred = df_pred_plot.toPandas().set_index('id')\n",
    "pddf_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Creates CSV of labeled data for later use (If needed) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf_pred.to_csv('LabeledDATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN KNN WITH K-FOLD CROSSVALIDATION For finding the optimal value of K (tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Applying Classification using supervised learnin K-NN\n",
    "X = pddf_pred[['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts']]\n",
    "y = pddf_pred['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "\n",
    "# range of k we want to try\n",
    "k_range = range(1, 41)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights = 'distance')\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = [0.9995939223997949, 0.9995939223997949, 0.9995605461099346, 0.9995800156639266, 0.9996940508052147, 0.9996606749021554, 0.9996690189746206, 0.9996801445335075, 0.999691269860314, 0.9996968323689968, 0.9997023951871205, 0.9997079578505238, 0.9997357712449002, 0.9997385525766017, 0.9997608033849351, 0.9997691474574003, 0.9997635848713571, 0.9997719289438223, 0.9997747101981636, 0.9997802726294862, 0.9997830541159083, 0.9997802728615669, 0.999777491452505, 0.9997747101981636, 0.999777491452505, 0.99976914738004, 0.9997747101208034, 0.9997802728615669, 0.9997913981110133, 0.9997969609291368, 0.9997969609291367, 0.9997941795974352, 0.9997997422608383, 0.9997969609291368, 0.9997997422608383, 0.9998080862559433, 0.9998025235151798, 0.9997997420287579, 0.9997941792106342, 0.9997913979562927]\n",
    "k_range = range(1, 41)\n",
    "\n",
    "# in essence, this is basically running the k-fold cross-validation method 40 times because we want to run through K values from 1 to 30\n",
    "# we should have 40 scores here\n",
    "print('Length of list', len(k_scores))\n",
    "print('Max of list', max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "# plt.plot(x_axis, y_axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6STF4jNXAVMm"
   },
   "source": [
    "# **TRAIN A CLASSIFIER ML BASED ALGORITHM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmSEMxkuIEHi"
   },
   "outputs": [],
   "source": [
    "###Applying Classification using supervised learnin K-NN\n",
    "x = pddf_pred[['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts']]\n",
    "y = pddf_pred['prediction']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyiMZkopI0av"
   },
   "outputs": [],
   "source": [
    "##Split data for training and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "uXcYMPdgMQFf",
    "outputId": "f88f8c2c-e884-4ac1-c908-e111c6db0a23"
   },
   "outputs": [],
   "source": [
    "##Find optimal K (KNN) value\n",
    "##Minimun Error\n",
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "  knn = KNeighborsClassifier(n_neighbors = i, weights = 'distance')\n",
    "  knn.fit(x_train, y_train)\n",
    "  pred_i = knn.predict(x_test)\n",
    "  error_rate.append(np.mean(pred_i != y_test))\n",
    " ## error_rate.append(metrics.accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color = 'blue',linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "print(\"Minimum error:=\",min(error_rate),\"at K=\",error_rate.index(min(error_rate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "zKqnt9sDPFJr",
    "outputId": "ce962c16-5dbe-408e-b7b0-49aa4f09e25c"
   },
   "outputs": [],
   "source": [
    "##Find optimal K (KNN) value\n",
    "##Maximun Accuracy\n",
    "accu = []\n",
    "for i in range(1,40):\n",
    "  knn = KNeighborsClassifier(n_neighbors = i, weights = 'distance')\n",
    "  knn.fit(x_train, y_train)\n",
    "  yhat = knn.predict(x_test)\n",
    " ## error_rate.append(np.mean(pred_i != y_test))\n",
    "  accu.append(metrics.accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),accu,color = 'blue',linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Accuracy vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "#print(\"Maximum accuracy:=\",max(accu),\"at K=\",accu.index(max(accu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Change the value of K to the best value from the tuning process, etiher K-Fold or Mean error and Accuracy technique from previous blocks **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UjngEWCST3i",
    "outputId": "1d4dead0-baad-44e2-f50d-5667bbf207b4"
   },
   "outputs": [],
   "source": [
    "##Run with optimal k\n",
    "k = 36\n",
    "knn = KNeighborsClassifier(n_neighbors = k, weights = 'distance')\n",
    "knn.fit(x_train, y_train)\n",
    "pred_y = knn.predict(x_test)\n",
    "print(\"Accuracy of model at K=36 is\", metrics.accuracy_score(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3nVZ3UcUCuU",
    "outputId": "e3612d12-b68a-4755-b078-b79ac8c31428"
   },
   "outputs": [],
   "source": [
    "len(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hW47K8jnUUNa",
    "outputId": "dbc5da13-36af-4934-cba5-1bf9b8412a8b"
   },
   "outputs": [],
   "source": [
    "pred_y\n",
    "print(type(pred_y))\n",
    "dfKNNPrediction = pd.DataFrame(pred_y, columns = ['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iD1tzfkcMnk3",
    "outputId": "aba7b3de-d8fe-4b02-b382-69ffdb9f6abf"
   },
   "outputs": [],
   "source": [
    "dfKNNPrediction.head()\n",
    "#len(dfKNNPrediction.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "35Fga1v_Ms3w",
    "outputId": "8d823636-f314-411b-998f-5ad1b0843390"
   },
   "outputs": [],
   "source": [
    "x_test.head()\n",
    "#len(x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8JyvjS_kMw9Y",
    "outputId": "702460c7-c84c-4461-a4cd-57a32d5d0aec"
   },
   "outputs": [],
   "source": [
    "x_testReset = x_test.reset_index()\n",
    "x_testComplete = x_testReset.drop(columns=['id'])\n",
    "x_testComplete.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "V0-zfRe6Mz-z",
    "outputId": "73c535ed-46be-497e-9234-6209dfbc1963"
   },
   "outputs": [],
   "source": [
    "x_testComplete[\"Prediction\"] = dfKNNPrediction[\"Prediction\"]\n",
    "x_testComplete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nxr5zkn7M-Ql",
    "outputId": "60965d9e-9f0e-4421-fda8-0b43fc6d43e8"
   },
   "outputs": [],
   "source": [
    "len(x_testComplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Saves teh KNN model to disk **\n",
    "** Place this model inside the python folder in your Apache Spark deployment **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'knn.sav'\n",
    "joblib.dump(knn, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxKbB2-LNSKp"
   },
   "source": [
    "# **TEST with normal traffic**\n",
    "# ** Change the name of the file to a CSV file containing only normal traffic streams **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHuOvvmrwDsV"
   },
   "source": [
    "**Firts we need to get the data and scale it using our previous scaler model that was trained on Original Monthly data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u36BoSmNasX",
    "outputId": "3f565d4c-ca8b-464a-cc37-087c22425a30"
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('nomalTraffic.csv', header=True)\n",
    "FEATURE_COLS = ['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts']\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = df.select(\"id\", FEATURE_COLS[0], FEATURE_COLS[1],FEATURE_COLS[2], FEATURE_COLS[3], FEATURE_COLS[4])\n",
    "for col in df.columns:\n",
    "\t  if col in FEATURE_COLS:\n",
    "\t\t    df = df.withColumn(col,df[col].cast('float'))\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURE_COLS, outputCol=\"features\")\n",
    "df_vector = vecAssembler.transform(df).select('id','features')  # Drop other columns.\n",
    "# Scale the data. We use a previous trained model from the original train data (To keep the ranges)\n",
    "df_scaledVector = scaler_model.transform(df_vector)\n",
    "df_scaledVector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRaz4XXnnwnS"
   },
   "outputs": [],
   "source": [
    "def extractVector(row):\n",
    "\t# From https://stackoverflow.com/questions/38384347/how-to-split-vector-into-columns-using-pyspark\n",
    "\treturn (row[\"id\"], row[\"scaledFeatures\"]) + tuple(row['scaledFeatures'].toArray().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67MwV1s7qygx",
    "outputId": "7a85fce1-a702-44e0-d208-73d0c5bcfa27"
   },
   "outputs": [],
   "source": [
    "scaleddf = df_scaledVector.rdd.map(extractVector).toDF([\"id\", \"scaledFeatures\", 'sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts'])\n",
    "scaleddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM_-n7Wkwzlf"
   },
   "source": [
    "**Convert to Pandas DF for further manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2xWp5CGPBoO",
    "outputId": "e4757ff8-06bd-4bb1-f0df-50dd9c6d0e8d"
   },
   "outputs": [],
   "source": [
    "dataNormal = scaleddf.toPandas()\n",
    "xNormal = dataNormal[['sumOfBytes', 'sumOfFlows', 'sumOfPackets', 'uniqDstIPs', 'uniqDstPorts']]\n",
    "len(xNormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQc73bvJPKSy",
    "outputId": "69a0a6c6-c294-4d5b-d276-1299529e5b45"
   },
   "outputs": [],
   "source": [
    "pred_yNormal = knn.predict(xNormal)\n",
    "pred_NormaPROB = knn.predict_proba(xNormal) #probabilites of classification\n",
    "pred_NormaPROB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nthFKIh4PSvS"
   },
   "source": [
    "**Join data with predictions for verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAQRU219PcRi"
   },
   "outputs": [],
   "source": [
    "classes = knn.classes_\n",
    "predpdNormal = pd.DataFrame(pred_yNormal, columns = ['Prediction'])\n",
    "predpdNormalPROB = pd.DataFrame(pred_NormaPROB, columns = classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "hRYxOIY2OCI2",
    "outputId": "e7b0b252-73fe-4d21-befa-e09130549704"
   },
   "outputs": [],
   "source": [
    "## PROB\n",
    "predpdNormalPROB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0eBQv0Vj9K-"
   },
   "source": [
    "**The output of the following code block are the normal clusters**\n",
    "** Save this output and include it in the list of Normal clusters in the anomalyDetection.py file **\n",
    "** The abnormal clusters are the remaining clusters that do not appear in this list **\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6q446ocjA8J",
    "outputId": "3986ed19-6cae-4da9-d635-a2f3bf6231df"
   },
   "outputs": [],
   "source": [
    "predpdNormal.Prediction.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "M8H_YHEwj8H7",
    "outputId": "9e4bfa94-f4cf-4b2a-bd91-5a9ce4fdefae"
   },
   "outputs": [],
   "source": [
    "\n",
    "xNormalComplete = xNormal.copy()\n",
    "xNormalComplete[\"Prediction\"] = predpdNormal[\"Prediction\"]\n",
    "xNormalComplete.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Mm6srsN_kNiQ",
    "3cR2S083jxfK",
    "NR9cxWB5HINF",
    "O2hrYZm-n_jL"
   ],
   "name": "KmeanspySparkipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
